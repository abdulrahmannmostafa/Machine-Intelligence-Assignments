from typing import Dict, Optional
from agents import Agent
from environment import Environment
from mdp import MarkovDecisionProcess, S, A
import json
from helpers.utils import NotImplemented


# This is a class for a generic Value Iteration agent
class ValueIterationAgent(Agent[S, A]):
    mdp: MarkovDecisionProcess[S, A]  # The MDP used by this agent for training
    utilities: Dict[S, float]  # The computed utilities
    # The key is the string representation of the state and the value is the utility
    discount_factor: float  # The discount factor (gamma)

    def __init__(
        self, mdp: MarkovDecisionProcess[S, A], discount_factor: float = 0.99
    ) -> None:
        super().__init__()
        self.mdp = mdp
        self.utilities = {
            state: 0 for state in self.mdp.get_states()
        }  # We initialize all the utilities to be 0
        self.discount_factor = discount_factor

    # Given a state, compute its utility using the bellman equation
    # if the state is terminal, return 0
    def compute_bellman(self, state: S) -> float:
        # TODO: Complete this function

        # Handle terminal state case
        if self.mdp.is_terminal(state):
            return 0.0

        # Compute the maximum expected utility over all actions available in the state and return the result
        return max(
            sum(
                prob
                * (
                    self.mdp.get_reward(state, action, next_state)
                    + self.discount_factor * self.utilities[next_state]
                )
                for next_state, prob in self.mdp.get_successor(state, action).items()
            )
            for action in self.mdp.get_actions(state)
        )

    # Applies a single utility update
    # then returns True if the utilities has converged (the maximum utility change is less or equal the tolerance)
    # and False otherwise
    def update(self, tolerance: float = 0) -> bool:
        # TODO: Complete this function
        max_change = 0.0
        new_utilities = (
            self.utilities.copy()
        )  # create a copy to store new utilities instead of updating in place
        for state in self.mdp.get_states():  # iterate over all states
            new_utility = self.compute_bellman(
                state
            )  # compute new utility using Bellman equation
            max_change = max(
                max_change, abs(new_utility - self.utilities[state])
            )  # track maximum change in utilities
            new_utilities[state] = new_utility  # update the new utility for each state
        self.utilities = new_utilities  # update the agent's utilities with the new computed utilities
        return max_change <= tolerance

    # This function applies value iteration starting from the current utilities stored in the agent and stores the new utilities in the agent
    # NOTE: this function does incremental update and does not clear the utilities to 0 before running
    # In other words, calling train(M) followed by train(N) is equivalent to just calling train(N+M)
    def train(self, iterations: Optional[int] = None, tolerance: float = 0) -> int:
        # TODO: Complete this function to apply value iteration for the given number of iterations
        iters = 0
        while True:
            iters += 1
            converged = self.update(
                tolerance
            )  # perform an update and check for convergence
            if converged:
                break
            if (
                iterations is not None and iters >= iterations
            ):  # check if max iterations reached
                break
        return iters  # return the number of iterations performed

    # Given an environment and a state, return the best action as guided by the learned utilities and the MDP
    # If the state is terminal, return None
    def act(self, env: Environment[S, A], state: S) -> A:
        # TODO: Complete this function

        # Handle terminal state case
        if self.mdp.is_terminal(state):
            return None

        best_action = None
        best_value = float("-inf")
        for action in self.mdp.get_actions(
            state
        ):  # iterate over all possible actions to find the best one thtat maximizes expected utility

            # Apply the Bellman equation to compute the expected utility of taking this action
            value = sum(
                prob
                * (
                    self.mdp.get_reward(state, action, next_state)
                    + self.discount_factor * self.utilities[next_state]
                )
                for next_state, prob in self.mdp.get_successor(state, action).items()
            )

            # Update best action if this action has a higher expected utility
            if value > best_value:
                best_value = value
                best_action = action

        return best_action  # return the action that maximizes expected utility

    # Save the utilities to a json file
    def save(self, env: Environment[S, A], file_path: str):
        with open(file_path, "w") as f:
            utilities = {
                self.mdp.format_state(state): value
                for state, value in self.utilities.items()
            }
            json.dump(utilities, f, indent=2, sort_keys=True)

    # loads the utilities from a json file
    def load(self, env: Environment[S, A], file_path: str):
        with open(file_path, "r") as f:
            utilities = json.load(f)
            self.utilities = {
                self.mdp.parse_state(state): value for state, value in utilities.items()
            }
